---
title: 'Analyzing Data from Within-Subjects Designs: Multivariate Approach vs. Linear
  Mixed Models Approach'
author: ''
date: '2019-02-13'
slug: analyzing-data-from-within-subjects-designs-multivariate-approach-vs-linear-mixed-models-approach
categories:
  - within-subjects designs
  - linear mixed effects models
  - multilevel models
  - contrasts
  - ANOVA
tags:
  - within-subjects designs
  - linear mixed effects models
  - multilevel models
  - contrasts
  - ANOVA
image:
  caption: ''
  focal_point: ''
---

Note I'm not done with this post yet. Don't trust my content here too much and lower expectations about how much I explain here!

# Within-Subjects Design
> In a within-subjects design, subjects give responses across multiple conditions or across time. In other words, measures are repeated across levels of some condition or across time points. For example, subjects can report how happy they feel when they see a sequence of positive pictures and another sequence of negative pictures. In this case, we'd observe each subjects' happiness in both positive and negative conditions. As another example, we could measure subjects' job satisifcation every month for 3 months. Here we'd observe job satisfaction for each subject across time. In both cases, subject scores across conditions or time very likely correlate with eachother -- they are dependent.

# What are contrasts?
> Broadly, contrasts test focused research hypotheses. A contrast comprises a set of weights or numeric values that represent some comparison. For example, when comparing two experimental group means (i.e., control vs. treatment), you can apply weights to each group mean and then sum them up. This is the same thing as subtracting one group's mean from the other's.

## Here's a quick demonstration

```{r}

# group means
control <- 5
treatment <- 3

# apply contrast weights and sum up the results
sum(c(control, treatment) * c(1, -1))

```

# Model and Conceptual Assumptions for Linear Regression
> * **Correct functional form.** Your model variables share linear relationships.  
> * **No omitted influences.** This one is hard: Your model accounts for all relevant influences on the variables included. All models are wrong, but how wrong is yours?  
> * **Accurate measurement.** Your measurements are valid and reliable. Note that unreliable measures can't be valid, and reliable measures don't necessairly measure just one construct or even your construct.  
> * **Well-behaved residuals.** Residuals (i.e., prediction errors) aren't correlated with predictor variables, predicted values, or eachother, and residuals have constant variance across values of your predictor variables or predicted values.  

# Model and Conceptual Assumptions for Repeated Measures ANOVA
> * **All change scores variances are equal.** Similar to the homogenous group variance assumption in between-subjects ANOVA designs, within-subjects designs require that all change score variances (e.g., subject changes between time points, within-subject differences between conditions) are equal. This means that if you compute the within-subject differences between all pairiwse levels (e.g., timepoints, treatment levels), the variances of those parwises differences must all be equal. For example, if you ask people how satisifed they are with their current job every month for 3 months, then the variance of month 2 - month 1 should equal the variance of month 3 - month 2 and the variance of month 3 - month 1. As you might be thinking, this assumption is very strict and probably not realistic in many cases.

# A different take on the homogenous change score variance assumption
> * **Sphericity and a special case, compound symmetry.** Sphericity is the matrix algebra equivalent to the homogenous change score variance assumption. Compound symmtry is a special case of sphericity. A variance-covariance matrix that satisifies compound symmetry has equal variances (the values in the diagonal) and equal covariances (the values above or below the diagonal).  
> **Short explanation**: Sphericity = homogenous change score variance = compound symmetry

# Install and load packages

```{r, warning = FALSE, message = FALSE}

# install.packages("tidyverse")
# install.packages("knitr")
# install.packages("lattice")
# install.packages("AMCP")
# install.packages("lme4")
# install.packages("lmerTest")

library(tidyverse)
library(knitr)
library(lattice)
library(AMCP)
library(lme4)
library(lmerTest)

```

## Print an example of a matrix that satisfies compound symmetry

```{r}

# 3 x 3 matrix, all values = 0.20
covmat <- matrix(0, nrow = 3, ncol = 3)

# for kicks, make all the variances = 0.50
diag(covmat) <- rep(0.50, 3)

# print
covmat %>% 
  as_tibble() %>% 
  mutate(x = factor(colnames(.), levels = colnames(.), ordered = TRUE)) %>% 
  gather(key = y, value = value, -x) %>% 
  mutate(y = factor(y, levels = unique(y), ordered = TRUE)) %>% 
  filter(!is.na(value)) %>% 
  ggplot(mapping = aes(x = x, y = y, fill = value, label = round(value, 2))) +
  geom_tile(color = "black") +
  geom_text() +
  scale_fill_gradient2(low = "#E41A1C", mid = "white", high = "#377EB8") +
  theme_bw()

```

# Multivariate approach
> every test uses a unique error term  
> calculate sum of squares, use those to estimate variance components and test statistics

# Linear Mixed Models Approach
> more flexible version of both univariate and multivariate approaches  
> estimate variances components directly, e.g. with maximum likelihood   
> can use all participant data even if some is missing

## Define custom oneway function (similar to [IBM SPSS's ONEWAY command](https://www.ibm.com/support/knowledgecenter/en/SSLVMB_24.0.0/spss/base/syn_oneway.html))

```{r}

oneway <- function(dv, group, contrast, alpha = .05) {
  # -- arguments --
  # dv: vector of measurements (i.e., dependent variable)
  # group: vector that identifies which group the dv measurement came from
  # contrast: list of named contrasts
  # alpha: alpha level for 1 - alpha confidence level
  # -- output --
  # computes confidence interval and test statistic for a linear contrast of population means in a between-subjects design
  # returns a data.frame object
  # estimate (est), standard error (se), t-statistic (z), degrees of freedom (df), two-tailed p-value (p), and lower (lwr) and upper (upr) confidence limits at requested 1 - alpha confidence level
  # first line reports test statistics that assume variances are equal
  # second line reports test statistics that do not assume variances are equal
  
  # means, standard deviations, and sample sizes
  ms <- by(dv, group, mean, na.rm = TRUE)
  vars <- by(dv, group, var, na.rm = TRUE)
  ns <- by(dv, group, function(x) sum(!is.na(x)))
  
  # convert list of contrasts to a matrix of named contrasts by row
  contrast <- matrix(unlist(contrast), nrow = length(contrast), byrow = TRUE, dimnames = list(names(contrast), NULL))
  
  # contrast estimate
  est <- contrast %*% ms
  
  # welch test statistic
  se_welch <- sqrt(contrast^2 %*% (vars / ns))
  t_welch <- est / se_welch
  
  # classic test statistic
  mse <- anova(lm(dv ~ factor(group)))$"Mean Sq"[2]
  se_classic <- sqrt(mse * (contrast^2 %*% (1 / ns)))
  t_classic <- est / se_classic
  
  # if dimensions of contrast are NULL, nummer of contrasts = 1, if not, nummer of contrasts = dimensions of contrast
  num_contrast <- ifelse(is.null(dim(contrast)), 1, dim(contrast)[1])
  df_welch <- rep(0, num_contrast)
  df_classic <- rep(0, num_contrast)
  
  # makes rows of contrasts if contrast dimensions aren't NULL
  if (is.null(dim(contrast))) contrast <- t(as.matrix(contrast))
  
  # calculating degrees of freedom for welch and classic
  for (i in 1:num_contrast) {
    df_classic[i] <- sum(ns) - length(ns)
    df_welch[i] <- sum(contrast[i, ]^2 * vars / ns)^2 / sum((contrast[i, ]^2 * vars / ns)^2 / (ns - 1))
  }
  
  # p-values
  p_welch <- 2 * (1 - pt(abs(t_welch), df_welch))
  p_classic <- 2 * (1 - pt(abs(t_classic), df_classic))
  
  # 95% confidence intervals
  lwr_welch <- est - se_welch * qt(p = 1 - (alpha / 2), df = df_welch)
  upr_welch <- est + se_welch * qt(p = 1 - (alpha / 2), df = df_welch)
  lwr_classic <- est - se_classic * qt(p = 1 - (alpha / 2), df = df_classic)
  upr_classic <- est + se_classic * qt(p = 1 - (alpha / 2), df = df_classic)
  
  # output
  data.frame(contrast = rep(rownames(contrast), times = 2),
             equal_var = rep(c("Assumed", "Not Assumed"), each = num_contrast),
             est = rep(est, times = 2),
             se = c(se_classic, se_welch),
             t = c(t_classic, t_welch),
             df = c(df_classic, df_welch),
             p = c(p_classic, p_welch),
             lwr = c(lwr_classic, lwr_welch),
             upr = c(upr_classic, upr_welch))
}

```

# Here's a simple demonstration using two responses from the same participants

## Load data
> From `help("sleep")`: "Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients."

```{r}

# create label
sleep$group_lbl <- with(sleep, ifelse(group == 1, "drug 1 vs. control", "drug 2 vs. control"))

# "The group variable name may be misleading about the data: They represent measurements on 10 persons, not in groups."
kable(sleep)

```

## Visualize relationships
> It's always a good idea to look at your data. Check some assumptions. 

```{r}

ggplot(sleep, mapping = aes(x = group_lbl, y = extra, color = group_lbl)) +
  geom_violin(alpha = 0) +
  geom_boxplot(outlier.color = NA, width = 0.25) +
  geom_point(position = position_jitter(width = 0.10), alpha = 0.50) +
  theme_bw()

```

## Test within-subjects effect using different approaches

### using `t.test(paired = TRUE)`

```{r}

t.test(extra ~ group, data = sleep, paired = TRUE)

```

### Compute a difference score and test whether the mean difference = 0

```{r}

# using dotproduct
t.test(with(sleep, cbind(extra[group == 1], extra[group == 2]) %*% c(-1, 1)))

# subtracting group 1 from group 2
t.test(with(sleep, extra[group == 1] * -1 + extra[group == 2] * 1))

```

### Fit a linear mixed effects model

```{r}

# create new contrast code called drug, which represents the drug effect on sleep increase
sleep$drug <- with(sleep, ifelse(group == 1, -0.5, 0.5))

# fit model
lmer.fit1 <- lmer(extra ~ drug + (1 | ID), data = sleep, REML = TRUE)

```

#### Diagnostic plots

```{r}

# residuals vs. fitted values
plot(lmer.fit1, type = c("p", "smooth"))

# q-q plot of random effects
qqmath(ranef(lmer.fit1, condVar = TRUE))

# q-q plot of fixed effects
qqmath(lmer.fit1)

# random effects estimates
dotplot(ranef(lmer.fit1, condVar = TRUE))

```

#### Results
> e.g., variance components, fixed effect coefficients, residual summary, fit indices

```{r}

summary(lmer.fit1)

```

# Here's a more complex situation with an additional between-subjects factor

## Generate data

```{r}

# set random seed
set.seed(8888)

# id
id <- factor(1:50)

# two treatment conditions, each size = 25
treatment <- factor(rep(1:2, each = 25))

# labels in a separate column
treatment_lbl <- ifelse(treatment == 1, "control", "treatment")

# draws random numbers from true populations, depending on treatment value
response1 <- rnorm(n = rep(25, 2)[treatment], mean = c(4, 5)[treatment], sd = c(1, 1)[treatment])

# response 2 should be correlated r = 0.5 with response 1, but the mean should be slightly higher
response2 <- response1 * 0.5 + rnorm(n = 50, mean = c(1, 2)[treatment], sd = 1)

# save and print data
mydata <- tibble(id, treatment_lbl, treatment, response1, response2)
kable(mydata)

# restructure
lmydata <- gather(mydata, key = sequence_lbl, value = response, response1:response2) %>% 
  mutate(sequence = ifelse(sequence_lbl == "response1", -0.5, 0.5),
         treatment = ifelse(treatment == 1, -0.5, 0.5))

```

## Visualize relationships
> It's always a good idea to look at your data. Check some assumptions. 

```{r}

ggplot(lmydata, mapping = aes(x = sequence_lbl, y = response, color = treatment_lbl)) +
  geom_violin(position = position_dodge(width = 0.90), alpha = 0) +
  geom_boxplot(outlier.color = NA, width = 0.25, position = position_dodge(width = 0.90)) +
  geom_point(position = position_jitterdodge(dodge.width = 0.90, jitter.width = 0.10), alpha = 0.40) +
  theme_bw()

```

## Test effects using different approaches

### Test average treatment effect (between-subjects)

```{r}

# create new columns that averages the responses
mydata$response_avg <- with(mydata, (response1 + response2) / 2)

# using t.test
t.test(response_avg ~ treatment_lbl, data = mydata)

# using oneway function
oneway(dv = mydata$response_avg, group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(-1, 1)), alpha = 0.05)

# using oneway function and dotproduct
oneway(dv = as.numeric(as.matrix(mydata[, c("response1", "response2")]) %*% c(1, 1) / 2), group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(-1, 1)), alpha = 0.05)

```

### Test sequence effect (within-subjects) via multivariate approach
> Average sequence effect will differ depending on whether other terms are included in the model

```{r}

# create new columns that reflects difference between two response
mydata$response_diff <- with(mydata, -response1 + response2)

# using t.test on difference score, compare to mean = 0
with(mydata, t.test(response_diff))

# using paired t.test
t.test(response ~ sequence_lbl, data = lmydata, paired = TRUE)

# using oneway function
oneway(dv = mydata$response_diff, group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(1, 1) / 2), alpha = 0.05)

# using oneway function and dotproduct
oneway(dv = as.numeric(as.matrix(mydata[, c("response1", "response2")]) %*% c(-1, 1)), group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(1, 1) / 2), alpha = 0.05)

```

### Test treatment by sequence effect via multivariate approach

```{r}

# using independent samples welch t.test
t.test(response_diff ~ treatment_lbl, data = mydata)

# using oneway function
oneway(dv = mydata$response_diff, group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(-1, 1)), alpha = 0.05)

# using oneway function and dotproduct
oneway(dv = as.numeric(as.matrix(mydata[, c("response1", "response2")]) %*% c(-1, 1)), group = mydata$treatment_lbl, contrast = list(treatment_lbl = c(-1, 1)), alpha = 0.05)

```

### Test treatment by sequence effect via linear mixed effects model

#### Fit a linear mixed effects model

```{r}

lmer.fit2 <- lmer(response ~ treatment * sequence + (1 + sequence | id), data = lmydata, REML = TRUE, control = lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore"))

```

#### Diagnostic plots

```{r}

# residuals vs. fitted values
plot(lmer.fit1, type = c("p", "smooth"))

# q-q plot of random effects
qqmath(ranef(lmer.fit1, condVar = TRUE))

# q-q plot of fixed effects
qqmath(lmer.fit1)

# random effects estimates
dotplot(ranef(lmer.fit1, condVar = TRUE))

```

#### Results
> e.g., variance components, fixed effect coefficients, residual summary, fit indices

```{r}

summary(lmer.fit2)

```

# Here's an example from the Maxwell, Dalaney, and Kelley (2017) textbook

## Load data
> From `help("C14T8")`: "For the hypothetical data contained in Table 14.8, a perceptual psychologist is interested in age differences ("young" and "old") in reaction time on a perceptual task. In addition, the psychologist is also interested in the effect of angle (zero degrees off center and eight degrees off center). The question of interest is to see if there are is a main effect of age, a main effect of angle, and an interaction between the two. Table 14.8 presents the same data that we analyzed in chapter 12 for 10 young participants and 10 old participants, except that for the moment we are only analyzing data from the 0 degree and 8 degree conditions of the angle factor."

```{r}

data("C14T8")

```

### Add angle4

```{r}

# there is a slight difference between the textbook data and the data available in the package -- I fix that here
C14T8 <- C14T8 %>%
  mutate(subj = factor(1:20),
         Angle4 = c(510, 480, 630, 660, 660, 450, 600, 660, 660, 540, 570, 720, 540, 660, 570, 780, 690, 570, 750, 690),
         age = recode(Group, "1" = -0.5, "2" = 0.5),
         age_lbl = recode(Group, "1" = "young", "2" = "old")) %>% 
  select(subj, Group, age, age_lbl, Angle0, Angle4, Angle8)

# print
kable(C14T8)

```

### Restructure data

```{r}

lC14T8 <- C14T8 %>%
  gather(key = angle, value = rt, Angle0, Angle4, Angle8) %>%
  mutate(angle_num = str_sub(angle, start = nchar(angle), end = nchar(angle)) %>% as.numeric(),
         angle_lbl = recode(angle_num, "0" = "0 degrees", "4" = "4 degrees", "8" = "8 degrees"),
         angle_linear = recode(angle_num, "0" = -0.5, "4" = 0, "8" = 0.5),
         angle_quadratic = recode(angle_num, "0" = 0.5, "4" = -1, "8" = 0.5))

```

### Visualize relationships
> It's always a good idea to look at your data. Check some assumptions. 

```{r}

ggplot(lC14T8, mapping = aes(x = angle_lbl, y = rt, color = age_lbl)) +
  geom_violin(position = position_dodge(width = 0.90), alpha = 0) +
  geom_boxplot(outlier.color = NA, width = 0.25, position = position_dodge(width = 0.90)) +
  geom_point(position = position_jitterdodge(dodge.width = 0.90, jitter.width = 0.10), alpha = 0.40) +
  theme_bw()

```

## Test effects using different approaches

### Test age effect (between-subjects) via multivariate approach
> c(1, 2, 3) %*% c(-1, 0, 1) means take the dot product of the matrix c(1, 2, 3) and the matrix c(-1, 0, 1), and this means multiply 1 by -1, 2 by 0, and 3 by 1, and then add up those products

```{r}

oneway(dv = as.numeric(as.matrix(C14T8[, c("Angle0", "Angle4", "Angle8")]) %*% (c(1, 1, 1) / 3)), group = C14T8$Group, contrast = list(age = c(-1, 1)))

```

### Test angle linear effect (within-subjects)
> c(1, 2, 3) %*% c(-1, 0, 1) means tke the dot product of the matrix c(1, 2, 3) and the matrix c(-1, 0, 1), and this this means multiply 1 by -1, 2 by 0, and 3 by 1, and then add up those products

```{r}

oneway(dv = as.numeric(as.matrix(C14T8[, c("Angle0", "Angle4", "Angle8")]) %*% c(-1, 0, 1)), group = C14T8$Group, contrast = list(age = c(1, 1) / 2))

```

### Test angle quadratic effect (within-subjects)
> c(1, 2, 3) %*% c(-1, 0, 1) means tke the dot product of the matrix c(1, 2, 3) and the matrix c(-1, 0, 1), and this this means multiply 1 by -1, 2 by 0, and 3 by 1, and then add up those products

```{r}

oneway(dv = as.numeric(as.matrix(C14T8[, c("Angle0", "Angle4", "Angle8")]) %*% c(1, -2, 1) / 3), group = C14T8$Group, contrast = list(age = c(1, 1) / 2))

```

### Test angle linear effect x age effect (mixed)
> c(1, 2, 3) %*% c(-1, 0, 1) means tke the dot product of the matrix c(1, 2, 3) and the matrix c(-1, 0, 1), and this this means multiply 1 by -1, 2 by 0, and 3 by 1, and then add up those products

```{r}

oneway(dv = as.numeric(as.matrix(C14T8[, c("Angle0", "Angle4", "Angle8")]) %*% c(-1, 0, 1)), group = C14T8$Group, contrast = list(age = c(-1, 1)))

```

### Test angle quadratic effect x age effect (mixed)
> c(1, 2, 3) %*% c(-1, 0, 1) means tke the dot product of the matrix c(1, 2, 3) and the matrix c(-1, 0, 1), and this this means multiply 1 by -1, 2 by 0, and 3 by 1, and then add up those products

```{r}

oneway(dv = as.numeric(as.matrix(C14T8[, c("Angle0", "Angle4", "Angle8")]) %*% c(1, -2, 1) / 3), group = C14T8$Group, contrast = list(age = c(-1, 1)))

```

### Test angle by age effects via linear mixed effects model

#### Fit a linear mixed effects model

```{r}

lmer.fit3 <- lmer(rt ~ (angle_linear + angle_quadratic) * age + (1 + angle_linear + angle_quadratic | subj), data = lC14T8, REML = TRUE, control = lmerControl(check.nobs.vs.nRE = "ignore", check.nobs.vs.nlev = "ignore"))

```

#### Diagnostic plots

```{r}

# residuals vs. fitted values
plot(lmer.fit3, type = c("p", "smooth"))

# q-q plot of random effects
qqmath(ranef(lmer.fit3, condVar = TRUE))

# q-q plot of fixed effects
qqmath(lmer.fit3)

# random effects estimates
dotplot(ranef(lmer.fit3, condVar = TRUE))

```

#### Results
> e.g., variance components, fixed effect coefficients, residual summary, fit indices

```{r}

summary(lmer.fit3)

```

# Export data for use in SPSS

```{r}

write_csv(lC14T8, path = "lC14T8.csv")

```

# Resources
> * Boik, R. J. (1981). A priori tests in repeated measures designs: Effects of nonsphericity. *Psychometrika, 46*(3), 241-255.  
> * Maxwell, S. E., Delaney, H. D., & Kelley, K. (2018). *Designing experiments and analyzing data: A model comparison perspective* (3rd ed.). Routledge.  
> * [DESIGNING EXPERIMENTS AND ANALYZING DATA](https://designingexperiments.com/) includes computer code for analyses from their book, Shiny apps, and more.  

# General word of caution
> Above, I listed resources prepared by experts on these and related topics. Although I generally do my best to write accurate posts, don't assume my posts are 100% accurate or that they apply to your data or research questions. Trust statistics and methodology experts, not blog posts.
